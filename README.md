
# ðŸ§  MNIST Digit Classifier â€” ERA Assignment

[![PyTorch](https://img.shields.io/badge/Framework-PyTorch-red)](https://pytorch.org/)
[![Device](https://img.shields.io/badge/Device-MPS%20|%20CUDA%20|%20CPU-blue)]()
[![Dataset](https://img.shields.io/badge/Dataset-MNIST-28x28-success)](http://yann.lecun.com/exdb/mnist/)

---

## ðŸ“Œ Project Overview

This project explores **lightweight CNN architectures for MNIST** under the following constraints:

* âœ… **<20K parameters**
* âœ… **BatchNorm & Dropout used**
* âœ… **Trained <20 epochs**
* âœ… **Validation/Test Accuracy â‰¥ 99.4%**

Final result: **99.6% test accuracy** with **14.5K parameters** in just **15 epochs** ðŸŽ¯

---

## ðŸ“Š Model at a Glance

* **Final Architecture:** Conv-BN-ReLU blocks + 1Ã—1 transition layers + Dropout + GAP head
* **Parameters:** \~14.5K
* **Accuracy:** 99.6% (test)
* **Head:** Global Average Pooling + 1Ã—1 Conv classifier
* **BatchNorm:** Yes (after each conv)
* **Dropout:** Yes (p=0.05 after each block)
* **Epochs:** 15
* **Scheduler:** OneCycleLR (per batch)

---

---

## ðŸ“ Layerwise Shape & Channels

| Layer  | In CÃ—HÃ—W | Kernel | Out CÃ—HÃ—W | Notes           |
| ------ | -------- | ------ | --------- | --------------- |
| Input  | 1Ã—28Ã—28  | â€“      | 1Ã—28Ã—28   | grayscale digit |
| Conv1  | 1Ã—28Ã—28  | 3Ã—3    | 16Ã—28Ã—28  | edge detectors  |
| Conv2  | 16Ã—28Ã—28 | 3Ã—3    | 16Ã—28Ã—28  | strokes/corners |
| Pool1  | 16Ã—28Ã—28 | 2Ã—2    | 16Ã—14Ã—14  | downsample      |
| Trans1 | 16Ã—14Ã—14 | 1Ã—1    | 12Ã—14Ã—14  | channel reduce  |
| Conv3  | 12Ã—14Ã—14 | 3Ã—3    | 28Ã—14Ã—14  | digit parts     |
| Conv4  | 28Ã—14Ã—14 | 3Ã—3    | 28Ã—14Ã—14  | refine features |
| Pool2  | 28Ã—14Ã—14 | 2Ã—2    | 28Ã—7Ã—7    | downsample      |
| Trans2 | 28Ã—7Ã—7   | 1Ã—1    | 32Ã—7Ã—7    | channel bump    |
| GAP    | 32Ã—7Ã—7   | â€“      | 32Ã—1Ã—1    | global summary  |
| Class  | 32Ã—1Ã—1   | 1Ã—1    | 10Ã—1Ã—1    | logits          |

---

## ðŸ” Change Log (Iterations)

| Iter | Params | Architecture Snapshot                          | Epochs | LR / Scheduler | Train Acc | Test Acc | What changed & why                                              |
| ---: | -----: | ---------------------------------------------- | -----: | -------------- | --------- | -------- | --------------------------------------------------------------- |
|   I0 | 18,060 | Tiny CNN, 2Ã—Convâ†’Poolâ†’2Ã—Convâ†’Pool, FC(1176â†’10) |      1 | 0.1 / StepLR   | 93.0%     | 98.0%    | Baseline; simple & fast but underfits a bit                     |
|   I1 | 20,978 | +BN,+Dropout; same FC head                     |     10 | 0.2 / OneCycle | 99.5%     | 99.6%    | BN stabilizes; Dropout regularizes; OneCycle speeds convergence |
|   I2 | 19,494 | +1Ã—1 transitions after pools                   |     12 | 0.2 / OneCycle | 99.6%     | 99.6%    | Channel compression improves parameter efficiency               |
|   I3 | 14,500 | GAP head (GAPâ†’1Ã—1 32â†’10), channels tuned       |     15 | 0.2 / OneCycle | 99.7%     | 99.6%    | Replace heavy FC with GAP; spend params on features             |

---

## ðŸ“ˆ Training Curves

*(Insert the loss/accuracy plots generated from your notebook here.)*

---

## âš¡ Why the Improvements Worked

* **BatchNorm**: stabilized training, allowed higher learning rates.
* **Dropout**: improved generalization by preventing overfitting.
* **1Ã—1 Convs (Transition layers)**: reduced parameter count while keeping depth.
* **GAP head**: replaced heavy FC, saving \~9K params; freed capacity for better feature maps.
* **OneCycleLR**: aggressive learning rate scheduling â†’ faster, smoother convergence.

---

## â–¶ï¸ How to Run

```bash
# clone repo
git clone https://github.com/yourusername/MNIST-ERA-Assignment.git
cd MNIST-ERA-Assignment

# create environment (example with uv)
uv venv venv
source venv/bin/activate

# install deps
pip install -r requirements.txt

# run notebook
jupyter notebook MNIST_U20K_99_4P.ipynb
```

---

## âœ… Compliance Checklist

* [x] <20K parameters (final \~14.5K)
* [x] BatchNorm used
* [x] Dropout used
* [x] GAP head (instead of FC)
* [x] <20 epochs (final run = 15)
* [x] Test accuracy â‰¥99.4% (achieved 99.6%)

---
