
# üß† MNIST Digit Classifier ‚Äî ERA Assignment

[![PyTorch](https://img.shields.io/badge/Framework-PyTorch-red)](https://pytorch.org/)
[![Device](https://img.shields.io/badge/Device-MPS%20|%20CUDA%20|%20CPU-blue)]()
[![Dataset](https://img.shields.io/badge/Dataset-MNIST-28x28-success)](http://yann.lecun.com/exdb/mnist/)

---

## üìå Project Overview

This project explores **lightweight CNN architectures for MNIST** under the following constraints:

* ‚úÖ **<20K parameters**
* ‚úÖ **BatchNorm & Dropout used**
* ‚úÖ **Trained <20 epochs**
* ‚úÖ **Validation/Test Accuracy ‚â• 99.4%**

Final result: **99.6% test accuracy** with **14.5K parameters** in just **15 epochs** üéØ

---

## üìä Model at a Glance

* **Final Architecture:** Conv-BN-ReLU blocks + 1√ó1 transition layers + Dropout + GAP head
* **Parameters:** \~14.5K
* **Accuracy:** 99.6% (test)
* **Head:** Global Average Pooling + 1√ó1 Conv classifier
* **BatchNorm:** Yes (after each conv)
* **Dropout:** Yes (p=0.05 after each block)
* **Epochs:** 15
* **Scheduler:** OneCycleLR (per batch)

---

## üèóÔ∏è Final Network Flow (Mermaid Diagram)

```mermaid
graph TD
  A[Input 1√ó28√ó28] --> B[Conv 3√ó3 1‚Üí16]
  B --> C[BN+ReLU]
  C --> D[Conv 3√ó3 16‚Üí16]
  D --> E[BN+ReLU]
  E --> F[MaxPool 2√ó2 28‚Üí14]
  F --> G[1√ó1 Conv 16‚Üí12]
  G --> H[Dropout 0.05]

  H --> I[Conv 3√ó3 12‚Üí28]
  I --> J[BN+ReLU]
  J --> K[Conv 3√ó3 28‚Üí28]
  K --> L[BN+ReLU]
  L --> M[MaxPool 2√ó2 14‚Üí7]
  M --> N[1√ó1 Conv 28‚Üí32]
  N --> O[Dropout 0.05]

  O --> P[GAP 7√ó7‚Üí1√ó1]
  P --> Q[1√ó1 Conv 32‚Üí10]
  Q --> R[Softmax (via CrossEntropy)]
```

---

## üìê Layerwise Shape & Channels

| Layer  | In C√óH√óW | Kernel | Out C√óH√óW | Notes           |
| ------ | -------- | ------ | --------- | --------------- |
| Input  | 1√ó28√ó28  | ‚Äì      | 1√ó28√ó28   | grayscale digit |
| Conv1  | 1√ó28√ó28  | 3√ó3    | 16√ó28√ó28  | edge detectors  |
| Conv2  | 16√ó28√ó28 | 3√ó3    | 16√ó28√ó28  | strokes/corners |
| Pool1  | 16√ó28√ó28 | 2√ó2    | 16√ó14√ó14  | downsample      |
| Trans1 | 16√ó14√ó14 | 1√ó1    | 12√ó14√ó14  | channel reduce  |
| Conv3  | 12√ó14√ó14 | 3√ó3    | 28√ó14√ó14  | digit parts     |
| Conv4  | 28√ó14√ó14 | 3√ó3    | 28√ó14√ó14  | refine features |
| Pool2  | 28√ó14√ó14 | 2√ó2    | 28√ó7√ó7    | downsample      |
| Trans2 | 28√ó7√ó7   | 1√ó1    | 32√ó7√ó7    | channel bump    |
| GAP    | 32√ó7√ó7   | ‚Äì      | 32√ó1√ó1    | global summary  |
| Class  | 32√ó1√ó1   | 1√ó1    | 10√ó1√ó1    | logits          |

---

## üîÅ Change Log (Iterations)

| Iter | Params | Architecture Snapshot                          | Epochs | LR / Scheduler | Train Acc | Test Acc | What changed & why                                              |
| ---: | -----: | ---------------------------------------------- | -----: | -------------- | --------- | -------- | --------------------------------------------------------------- |
|   I0 | 18,060 | Tiny CNN, 2√óConv‚ÜíPool‚Üí2√óConv‚ÜíPool, FC(1176‚Üí10) |      1 | 0.1 / StepLR   | 93.0%     | 98.0%    | Baseline; simple & fast but underfits a bit                     |
|   I1 | 20,978 | +BN,+Dropout; same FC head                     |     10 | 0.2 / OneCycle | 99.5%     | 99.6%    | BN stabilizes; Dropout regularizes; OneCycle speeds convergence |
|   I2 | 19,494 | +1√ó1 transitions after pools                   |     12 | 0.2 / OneCycle | 99.6%     | 99.6%    | Channel compression improves parameter efficiency               |
|   I3 | 14,500 | GAP head (GAP‚Üí1√ó1 32‚Üí10), channels tuned       |     15 | 0.2 / OneCycle | 99.7%     | 99.6%    | Replace heavy FC with GAP; spend params on features             |

---

## üìà Training Curves

*(Insert the loss/accuracy plots generated from your notebook here.)*

---

## ‚ö° Why the Improvements Worked

* **BatchNorm**: stabilized training, allowed higher learning rates.
* **Dropout**: improved generalization by preventing overfitting.
* **1√ó1 Convs (Transition layers)**: reduced parameter count while keeping depth.
* **GAP head**: replaced heavy FC, saving \~9K params; freed capacity for better feature maps.
* **OneCycleLR**: aggressive learning rate scheduling ‚Üí faster, smoother convergence.

---

## ‚ñ∂Ô∏è How to Run

```bash
# clone repo
git clone https://github.com/yourusername/MNIST-ERA-Assignment.git
cd MNIST-ERA-Assignment

# create environment (example with uv)
uv venv venv
source venv/bin/activate

# install deps
pip install -r requirements.txt

# run notebook
jupyter notebook MNIST_U20K_99_4P.ipynb
```

---

## ‚úÖ Compliance Checklist

* [x] <20K parameters (final \~14.5K)
* [x] BatchNorm used
* [x] Dropout used
* [x] GAP head (instead of FC)
* [x] <20 epochs (final run = 15)
* [x] Test accuracy ‚â•99.4% (achieved 99.6%)

---
